# house-price-prediction-ames
Summary:
1. Data Reading and EDA
I have read the data and performed basic EDA to gain insights. Duplicate rows have been checked.Scatterplots for all predictors vs Target variable will show the presence of linearity/non-linearity. Heatmaps show the top correlated features with out target variable. We check for the presence of skewness in these variables and also our target variable. We will later apply log and boxcox transformation on them.
2. Data Cleaning
I have refered to the following source for feature engineering http://jse.amstat.org/v19n3/decock/DataDocumentation.txt Outliers have been removed as per the document, but most have been preserved to minimise data loss. Missing values have been replaced with a separate category or replaced with zero, again following the document. Target variable is log transformed.
3. Feature Engineering
This is the most crucial step, as I derive more useful features from existing ones which will improve our model. The skewed columns are found and boxcox transformation is applied. Also, some categorical features that are coded as numerical values, are given ordinality through label encoding. This will help the model learn efficiently. We derive more information from the missing values that were encoded as a separate category or zero by creating binary columns. We also create derived features from two or more independent features which provide more insight to the data. Polynomial Feature transformation is applied on the most correlated variables with Target variable. Robust scaling is chosen as it handles outliers better. Finally one hot encoding is performed on the categorical variables.
4. Model building
We split the data into train and test data.We chose regularization techniques like ridge and lasso to prevent overfitting our data as there are huge number of columns. We check that lasso provides a very high r2 score with extremely small lambda through crossvalidation. Ridge does not reduce the coefficients to zero and performs well above a lambda of 10. We train the model and r2 for the train data comes to around 94 percent. We see that lasso has retained only a small fraction of the variables. The model is able to explain 91% of the variation in test data. The plot of residuals vs predicted values does not show any pattern and is thus free from heteroscedacity and the error terms are independent. The errors also follow a gaussian distribution
